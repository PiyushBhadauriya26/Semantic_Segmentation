{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ae7bbe-f9e0-4eec-bd19-4890fa7dbd5f",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<div>\n",
    "<img src=\"../assets/gtMsk.png\" width=\"250\"/>\n",
    "<img src=\"../assets/samAutoMsk.png\" width=\"250\"/>\n",
    "<img src=\"../assets/samPointPrompt.png\" width=\"250\"/>\n",
    "<img src=\"../assets/samBoxPrompt1.png\" width=\"250\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "**<summary>What is the Segment Anything Model (SAM)?</summary>**\n",
    "The Segment Anything Model (SAM) is a segmentation model developed by Meta AI. It is considered the first foundational model for Computer Vision. SAM was trained on a huge corpus of data containing millions of images and billions of masks, making it extremely powerful. As its name suggests, SAM is able to produce accurate segmentation masks for a wide variety of images. SAMâ€™s design allows it to take human prompts into account, making it particularly powerful for Human In The Loop annotation. These prompts can be multi-modal: they can be points on the area to be segmented, a bounding box around the object to be segmented or a text prompt about what should be segmented.\n",
    "\n",
    "\n",
    "**<summary>Fine tuning SAM with Low-Rank Adaptation (LoRA)?</summary>**\n",
    "LoRA is an adapter that is using 2 matrices B and A. The 2 matrices have specific dimensions (input_size, r) and (r, input_size) . By specifying a rank r < input_size, we reduce the parameters size and try to capture the task with a small enough rank. The matrix product B*A gives a matrix of shape (input_size, input_size) so no information is lost but the model will have learned a new representation through training.\n",
    "\n",
    "For any application, we only need to initialize the matrices, freeze SAM and train the adapter so that the frozen model + LoRA learns to segment anythings that you need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8455ef19-aef2-4ea8-90b5-08e0817b2563",
   "metadata": {},
   "source": [
    "## Data PreProcessing \n",
    "BCSS dataset masks_orig has 22 different classes of cells \n",
    "```\n",
    "outside_roi             0\n",
    "tumor\t                1\n",
    "stroma\t                2\n",
    "lymphocytic_infiltrate  3\n",
    "necrosis_or_debris      4\n",
    "glandular_secretions    5\n",
    "blood                   6\n",
    "exclude                 7\n",
    "metaplasia_NOS          8\n",
    "fat                     9\n",
    "plasma_cells            10\n",
    "other_immune_infiltrate\t11\n",
    "mucoid_material\t        12\n",
    "normal_acinus_or_duct\t13\n",
    "lymphatics              14\n",
    "undetermined\t        15\n",
    "nerve\t                16\n",
    "skin_adnexa             17\n",
    "blood_vessel\t        18\n",
    "angioinvasion\t        19\n",
    "dcis\t                20\n",
    "other\t                21\n",
    "```\n",
    "Convert it to 2 classes\n",
    "```\n",
    "no tumor                0\n",
    "tumor                   1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d97776a8-ae4a-4156-a61b-92cc39c321dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "\n",
    "in_folder = \"../data/BCSS_small/train/masks_orig/\"\n",
    "out_folder = \"../data/BCSS_small/train/masks/\"\n",
    "overwrite = False\n",
    "\n",
    "_mask_transformer = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "_image_transformer = transforms.Compose([\n",
    "    transforms.ToPILImage()\n",
    "])\n",
    "\n",
    "def process_and_save_mask(image_file):\n",
    "    mask = Image.open(os.path.join(in_folder,image_file))\n",
    "    mask_tensor = _mask_transformer(mask)\n",
    "    mask_tensor = (mask_tensor == 1).to(torch.uint8) # convert to 0/1 mask\n",
    "    mask_image = _image_transformer(mask_tensor)\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    mask_image.save(os.path.join(out_folder, image_file))\n",
    "\n",
    "in_masks = [file for file in os.listdir(in_folder) if file.endswith('.png')]\n",
    "for image_file in in_masks:\n",
    "    if os.path.isfile(os.path.join(out_folder, image_file)) and not overwrite:\n",
    "        continue\n",
    "    process_and_save_mask(image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296255f3-b233-4ead-9769-559f1016e8a4",
   "metadata": {},
   "source": [
    "#### Original Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a54e8e33-1ff6-4da6-ad24-242b68095c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1, 2], dtype=torch.uint8), tensor([31460, 18716]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[2, 2, 2,  ..., 2, 2, 2],\n",
       "         [2, 2, 2,  ..., 1, 1, 1],\n",
       "         [2, 2, 2,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [2, 2, 2,  ..., 2, 2, 2],\n",
       "         [2, 2, 2,  ..., 2, 2, 2],\n",
       "         [2, 2, 2,  ..., 2, 2, 2]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = Image.open(os.path.join(in_folder,in_masks[0]))\n",
    "mask_tensor = _mask_transformer(mask)\n",
    "print(mask_tensor.unique(return_counts=True))\n",
    "mask_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfc1cff-25e3-4d1f-94de-bfba11a8558c",
   "metadata": {},
   "source": [
    "#### Processed Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5edd62c8-c0a0-4e38-b505-9bfbf1f9e67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 1], dtype=torch.uint8), tensor([18716, 31460]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         [0, 0, 0,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = Image.open(os.path.join(out_folder,in_masks[0]))\n",
    "mask_tensor = _mask_transformer(mask)\n",
    "print(mask_tensor.unique(return_counts=True))\n",
    "mask_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a8820-8e69-42ed-b29b-c6a83c2efbe3",
   "metadata": {},
   "source": [
    "## Code Repo and Setup\n",
    "https://github.com/PiyushBhadauriya26/Semantic_Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ec6d252-b1cb-4f37-9c5d-39a25c893f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Setup\n",
       "- Use Python 3.12\n",
       "- Use virtual env\n",
       "- Install pytorch https://pytorch.org/get-started/locally/#windows-python\n",
       "- Use requirements.txt to install the required python libs\n",
       "\n",
       "#### Server\n",
       "- Start the server `python LitServe_SAM.py`\n",
       "- Health check `http://localhost:8000/health`\n",
       "\n",
       "#### Client \n",
       "Use client code to call /predict API when the server is up.\n",
       "\n",
       "```\n",
       "usage: client.py [-h] --image IMAGE [--p1 P1] [--p2 P2] [--model MODEL] [--alpha ALPHA]\n",
       "\n",
       "Send text & image to server and receive a response.\n",
       "\n",
       "optional arguments:  \n",
       "  -h, --help     show this help message and exit  \n",
       "  --image IMAGE  URL for the image file.  \n",
       "  --p1 P1        Single Point input in '(x,y)' format.\n",
       "  --p2 P2        Point2 '(x1,y1)' for box input.\n",
       "  --model MODEL  Name of the model [sam-vit_l, sam-vit_h, med_sam-vit_b, sam-vit_b-lora512]\n",
       "  --alpha ALPHA  Transparency mask between 0-1.\n",
       "```\n",
       "response contains segmented image with identified region with no mask and green mask for background region.\n",
       "###### Example\n",
       "- `python client.py --image .\\data\\test1.png --p1 \"(60,40)\" --p2 \"(180,120)\" --alpha 0.8`# For best results provide box input with Region of interest.\n",
       "- `python client.py --image .\\data\\test1.png --alpha 0.5 --model \"med_sam-vit_b\"` # Use med_sam-vit_b model to segmentation whole image  \n",
       "- `python client.py --image .\\data\\test1.png --p1 \"(130,80)\" --alpha 0.5 --model \"med_sam-vit_b\"` # Point input \n",
       "\n",
       "#### Batch Inference \n",
       "Use `batch_inference.py` script to run inference on multiple images\n",
       "```\n",
       "usage: batch_inference.py [-h] [-i DATA_PATH] [-o SEG_PATH] [--device DEVICE] [--overwrite OVERWRITE] [--model MODEL]\n",
       "\n",
       "run inference on testing set based on MedSAM\n",
       "\n",
       "options:\n",
       "  -h, --help                               show this help message and exit\n",
       "  -i DATA_PATH, --data_path DATA_PATH      path to the data folder\n",
       "  -o SEG_PATH, --seg_path SEG_PATH         path to the segmentation folder\n",
       "  --device DEVICE                          Device cuda or cpu\n",
       "  --overwrite OVERWRITE                    Overwrite existing results with a new mask.\n",
       "  --model MODEL                            Name of the model [sam-vit_l, sam-vit_h, med_sam-vit_b, sam-vit_b-lora512]\n",
       "\n",
       "```\n",
       "###### Example\n",
       "- `python .\\batch_inference.py -i \"data/BCSS_small/test/images\" --overwrite True` # Run inference and save predicted masks in data/Results folder\n",
       "\n",
       "#### Train\n",
       "- Source: https://github.com/WangRongsheng/SAM-fine-tune\n",
       "- Update `config.yaml` for DATASET paths, CHECKPOINT for base sam model and TRAIN setting\n",
       "- `python train.py` # Start training.\n",
       "- After training lora weights are saved as safetensors file in model_checkpoint folder\n",
       "\n",
       "#### References\n",
       "1. https://github.com/facebookresearch/segment-anything\n",
       "2. https://github.com/facebookresearch/sam2\n",
       "3. https://github.com/bowang-lab/MedSAM\n",
       "4. https://github.com/mazurowski-lab/finetune-SAM\n",
       "5. https://github.com/WangRongsheng/SAM-fine-tune\n",
       "6. https://github.com/Lightning-AI/LitServe\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(\"../README.md\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c886f2-f406-4ef1-9b48-59d5216031bf",
   "metadata": {},
   "source": [
    "## Metrics And Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483bb59-3945-4b4d-9c7d-32bb08d80c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic_segmentation",
   "language": "python",
   "name": "semantic_segmentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
